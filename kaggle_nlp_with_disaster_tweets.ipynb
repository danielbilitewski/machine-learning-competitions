{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import spacy for NLP and re for regular expressions\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "\n",
    "# import sklearn transformers, models and pipelines\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# import distributions for randomized grid search\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Load the small language model from spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# set pandas text output to 400\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Bag of Words & Naive Bayes\n",
    "## Load and Prepare Data\n",
    "There are duplicate rows. Some have the same text and target, while others only have the same text but different target. \n",
    "\n",
    "For those with the same target, only one of the duplicate rows should be kept in order to only have unique observations.\n",
    "\n",
    "For those rows with the same text and different target, it is better to drop all rows, as it would be hard to manually relabel the rows or to check for the correct label.\n",
    "\n",
    "We can see that this way 128 rows are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7613, 5)\n",
      "Test shape: (3263, 4)\n",
      "Sample submission shape: (3263, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n",
    "\n",
    "# print shape of datasets\n",
    "print('Train shape: {}'.format(train.shape))\n",
    "print('Test shape: {}'.format(test.shape))\n",
    "print('Sample submission shape: {}'.format(sample_submission.shape))\n",
    "\n",
    "# inspect train set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7485, 5)\n"
     ]
    }
   ],
   "source": [
    "# find duplicate rows with same text and target, keep only the first\n",
    "train.drop_duplicates(subset = ['text', 'target'], inplace = True)\n",
    "\n",
    "# some rows have the same text, but different targets\n",
    "# drops all of these rows\n",
    "train.drop_duplicates(subset = 'text', keep = False, inplace = True)\n",
    "\n",
    "# print new shape of train set\n",
    "print('Train shape: {}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Create Machine Learning Pipeline\n",
    "The first step is creating a machine learning pipeline using the `make_pipeline`function from scikit-learn. Creating a pipeline is important to have a robust workflow. For example, it ensures that all preprocessing steps that are learned on data are done within the cross-validation, to ensure that no data is leaked to the model. \n",
    "\n",
    "In this case, I'm using a `CountVectorizer` to turn the text into a high-dimensional sparse matrix. It uses a bag of words approach, where the bag of words contains each word in the entire train set. This will be the columns of the matrix. Then, for each row corresponding to a tweet, if the word is within the tweet it will have the entry 1, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create machine learning pipeline\n",
    "nb_pipe = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "The next step is to create a baseline model. This is just doing a cross-validation on the raw train set using the pipeline created before, without any other data preparation steps. It serves to verify how well the data preparation steps improve the model performance, if at all. \n",
    "\n",
    "One important thing to note here is that there is a large discrepancy between the scores I achieved in cross-validation and the scores achieved on the public leaderboard. In this case, the baseline model scores around 0.795 on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train set, test set and target\n",
    "X_train = train.text\n",
    "X_test = test.text\n",
    "y_train = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.672\n"
     ]
    }
   ],
   "source": [
    "# cross validate\n",
    "print('F1 score: {:.3f}'.format(np.mean(cross_val_score(nb_pipe, X_train, y_train, scoring = 'f1'))))\n",
    "\n",
    "# fit pipeline\n",
    "nb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "pred = nb_pipe.predict(X_test)\n",
    "\n",
    "# submit prediction\n",
    "sample_submission.target = pred\n",
    "sample_submission.to_csv('naive_bayes_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization is a text proprocessing technique that gets the lemma for each word, which is basically like a root of the word. The advantage of this technique is that different variations of the same word will have the same lemma and will therefore be considered the same in the bag of words. This should improve the learning and generalization ability of the model.\n",
    "\n",
    "Lemmatization doesn't require tokenization before. The reason that I have a seperate function for tokenization is that I use it to remove stop words, which are words that appear so commonly that they don't carry any meaning or predictive power in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string, stop_words):\n",
    "    \"\"\"\n",
    "    Tokenize a document passed as a string, remove stop words and \n",
    "    return all tokens as a single document in the same order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a document object\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # Generate tokens\n",
    "    tokens_with_stopwords = [token.text for token in doc]\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens_with_stopwords if token not in stop_words]\n",
    "\n",
    "    # Convert tokens into a string and return it\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def lemmatize(string):\n",
    "    \"\"\"\n",
    "    Lemmatize a document passed as a string and return all lemmas as a document in the same order.\n",
    "    \"\"\"\n",
    "    # Create a document object\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # Generate tokens\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "    # Convert tokens into a string and return it\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# tokenize the train and test set\n",
    "X_train = X_train.apply(tokenize, stop_words = STOP_WORDS)\n",
    "X_test = X_test.apply(tokenize, stop_words = STOP_WORDS)\n",
    "\n",
    "# lemmatize the train and test set\n",
    "X_train = X_train.apply(lemmatize)\n",
    "X_test = X_test.apply(lemmatize)\n",
    "\n",
    "# create target\n",
    "y_train = train.target.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.665\n"
     ]
    }
   ],
   "source": [
    "# cross validate\n",
    "print('F1 score: {:.3f}'.format(np.mean(cross_val_score(nb_pipe, X_train, y_train, scoring = 'f1'))))\n",
    "\n",
    "# fit pipeline\n",
    "nb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "pred = nb_pipe.predict(X_test)\n",
    "\n",
    "# submit prediction\n",
    "sample_submission.target = pred\n",
    "sample_submission.to_csv('naive_bayes_spacy_pipeline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Customizing the NLP Pipeline\n",
    "After applying tokenization, we can see that there are still many elements in the texts which don't generalize well. This includes for example hyperlinks, mentions or numbers. I create a custom proprocessing function using regular expression to replace these by placeholder words which will be the same across all tweets. For example, instead of a hyperlink, the tweets will now contain the word HYPERLINK. \n",
    "\n",
    "The logic behind this is that it might not matter where the link goes to and that there won't be any generalization because it's unlikely that two tweets will have the same hyperlinks. Instead, it might be just interesting to see that a tweet has a hyperlink. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized tweets: --------------------\n",
      "\n",
      "0                                                                               Our Deeds Reason # earthquake May ALLAH Forgive\n",
      "1                                                                                       Forest fire near La Ronge Sask . Canada\n",
      "2                         All residents asked ' shelter place ' notified officers . No evacuation shelter place orders expected\n",
      "3                                                                13,000 people receive # wildfires evacuation orders California\n",
      "4                                                              Just got sent photo Ruby # Alaska smoke # wildfires pours school\n",
      "                                                                 ...                                                           \n",
      "7604    # WorldNews Fallen powerlines G : link tram : UPDATE : FIRE crews evacuated 30 passengers tr ... http://t.co/EYSVvzA7Qm\n",
      "7605                                                                             flip I Walmart bomb evacuate stay tuned I blow\n",
      "7606         Suicide bomber kills 15 Saudi security site mosque - Reuters World - Google News - Wall ... http://t.co/nF4IculOje\n",
      "7608                                               Two giant cranes holding bridge collapse nearby homes http://t.co/STfMbbZFB5\n",
      "7612                               The Latest : More Homes Razed Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d\n",
      "Name: text, Length: 7485, dtype: object\n",
      "\n",
      "Preprocessed tweets: --------------------\n",
      "\n",
      "0                                                                                        Our Deeds Reason HASHTAG earthquake May ALLAH Forgive\n",
      "1                                                                                                 Forest fire near La Ronge Sask NUMBER Canada\n",
      "2                                       All residents asked shelter place notified officers NUMBER No evacuation shelter place orders expected\n",
      "3                                                                         NUMBER people receive HASHTAG wildfires evacuation orders California\n",
      "4                                                                 Just got sent photo Ruby HASHTAG Alaska smoke HASHTAG wildfires pours school\n",
      "                                                                         ...                                                                  \n",
      "7604    HASHTAG WorldNews Fallen powerlines G NUMBER link tram NUMBER UPDATE NUMBER FIRE crews evacuated NUMBER passengers tr NUMBER HYPERLINK\n",
      "7605                                                                                            flip I Walmart bomb evacuate stay tuned I blow\n",
      "7606                              Suicide bomber kills NUMBER Saudi security site mosque - Reuters World - Google News - Wall NUMBER HYPERLINK\n",
      "7608                                                                           Two giant cranes holding bridge collapse nearby homes HYPERLINK\n",
      "7612                                                      The Latest NUMBER More Homes Razed Northern California Wildfire - ABC News HYPERLINK\n",
      "Name: text, Length: 7485, dtype: object\n",
      "\n",
      "Lemmatized preprocessed tweets: --------------------\n",
      "\n",
      "0                                                                                 -PRON- Deeds Reason HASHTAG earthquake May ALLAH forgive\n",
      "1                                                                                             forest fire near La Ronge Sask NUMBER Canada\n",
      "2                                            all resident ask shelter place notify officer number no evacuation shelter place order expect\n",
      "3                                                                       number people receive HASHTAG wildfire evacuation order California\n",
      "4                                                               just get send photo Ruby HASHTAG Alaska smoke HASHTAG wildfire pour school\n",
      "                                                                       ...                                                                \n",
      "7604    HASHTAG WorldNews Fallen powerline g number link tram number UPDATE number fire crew evacuate number passenger tr NUMBER hyperlink\n",
      "7605                                                                               flip -PRON- Walmart bomb evacuate stay tune -PRON- blow\n",
      "7606                           suicide bomber kill number saudi security site mosque - Reuters World - Google News - Wall NUMBER HYPERLINK\n",
      "7608                                                                            two giant crane hold bridge collapse nearby home HYPERLINK\n",
      "7612                                                      the late number More home raze Northern California Wildfire - ABC News HYPERLINK\n",
      "Name: text, Length: 7485, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenize(string, stop_words):\n",
    "    \"\"\"\n",
    "    Tokenize a document passed as a string, remove stop words and \n",
    "    return all tokens as a single document in the same order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a document object\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # Generate tokens\n",
    "    tokens_with_stopwords = [token.text for token in doc]\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens_with_stopwords if token not in stop_words]\n",
    "\n",
    "    # Convert tokens into a string and return it\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess(series):\n",
    "    \"\"\"\n",
    "    Function to clean the tweets by replacing words or characters with little meaning.\n",
    "    \n",
    "    Replaces all hyperlinks, numbers, mentions and hashtags with a single identifier \n",
    "    (e.g. 'https://google.com' becomes 'HYPERLINK')\n",
    "    \n",
    "    Replaces special characters such as exclamation marks, question marks, quotation marks and brackets.\n",
    "    \n",
    "    Replaces double or more white spaces with a single white space.\n",
    "    \"\"\"\n",
    "    # replace all hyperlinks\n",
    "    series = series.map(lambda string: re.sub(r'http.*', 'HYPERLINK', string))\n",
    "\n",
    "    # replace all numbers\n",
    "    series = series.map(lambda string: re.sub(r'[0-9,.:]+', 'NUMBER', string))\n",
    "\n",
    "    # replace all mentions\n",
    "    series = series.map(lambda string: re.sub(r'@\\w+', 'MENTION', string))\n",
    "\n",
    "    # replace all hashtags\n",
    "    series = series.map(lambda string: re.sub(r'#', 'HASHTAG', string))\n",
    "\n",
    "    # replace all symbols\n",
    "    series = series.map(lambda string: re.sub(r\"[\\!\\?\\'\\\"\\{\\[\\(\\)\\]\\}]\", '', string))\n",
    "\n",
    "    # replace all double space or more with a single space\n",
    "    series = series.map(lambda string: re.sub(r'[ ][ ]+', ' ', string))\n",
    "    \n",
    "    # return series\n",
    "    return series\n",
    "\n",
    "# tokenize the text\n",
    "X_train = train.text.apply(tokenize, stop_words = STOP_WORDS)\n",
    "X_test = test.text.apply(tokenize, stop_words = STOP_WORDS)\n",
    "\n",
    "print('Tokenized tweets: --------------------\\n')\n",
    "print(X_train)\n",
    "\n",
    "# preprocess the train and test set\n",
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)\n",
    "\n",
    "print('\\nPreprocessed tweets: --------------------\\n')\n",
    "print(X_train)\n",
    "\n",
    "# lemmatize the train and test set\n",
    "X_train = X_train.apply(lemmatize)\n",
    "X_test = X_test.apply(lemmatize)\n",
    "\n",
    "print('\\nLemmatized preprocessed tweets: --------------------\\n')\n",
    "print(X_train)\n",
    "\n",
    "# create target\n",
    "y_train = train.target.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.662\n"
     ]
    }
   ],
   "source": [
    "# cross validate\n",
    "print('F1 score: {:.3f}'.format(np.mean(cross_val_score(nb_pipe, X_train, y_train, scoring = 'f1'))))\n",
    "\n",
    "# fit pipeline\n",
    "nb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "pred = nb_pipe.predict(X_test)\n",
    "\n",
    "# submit prediction\n",
    "sample_submission.target = pred\n",
    "sample_submission.to_csv('naive_bayes_custom_pipeline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "The last step is hyperparameter tuning to get the most out of the model with the existing data preparation steps. I tune the following hyperparameters:\n",
    "\n",
    "CountVectorizer: I see if replacing the `CountVectorizer` by the `TfidfVectorizer` improves the performance. The `TfidfVectorizer` works similar to the `CountVectorizer`, with the only difference that it ways words based on how frequent they appear in the dataset. The more frequent they appear, the less informative they are considered to be.\n",
    "\n",
    "N-grams: N-grams consider combination of words that follow each other. This provides more context but also creates a much larger bag of words, reducing the generalization power of the model. \n",
    "\n",
    "Minimum document frequency: Words that appear only once in the document also don't have as much generalization power, as we would need a word to appear at least twice to learn something meaningful.\n",
    "\n",
    "Naive Bayes alpha: The alpha is a smoothing parameter for the probabilities. Understanding how this works requires more in-depth knowledge about the math behind the Naive Bayes algorithm. For this purpose it's enough to know that the optimal range will usually be between 0.7 and 1.0 but the effect on model performance is usually low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_countvectorizer</th>\n",
       "      <th>param_countvectorizer__min_df</th>\n",
       "      <th>param_countvectorizer__ngram_range</th>\n",
       "      <th>param_multinomialnb__alpha</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.814801</td>\n",
       "      <td>0.662417</td>\n",
       "      <td>0.879288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.895266</td>\n",
       "      <td>0.662371</td>\n",
       "      <td>0.876992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.893602</td>\n",
       "      <td>0.662371</td>\n",
       "      <td>0.877074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.955471</td>\n",
       "      <td>0.662164</td>\n",
       "      <td>0.875355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.947308</td>\n",
       "      <td>0.662128</td>\n",
       "      <td>0.875633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.712271</td>\n",
       "      <td>0.662099</td>\n",
       "      <td>0.883438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.890044</td>\n",
       "      <td>0.662024</td>\n",
       "      <td>0.877167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.86904</td>\n",
       "      <td>0.661703</td>\n",
       "      <td>0.877818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.842561</td>\n",
       "      <td>0.661677</td>\n",
       "      <td>0.878586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>0.728939</td>\n",
       "      <td>0.661635</td>\n",
       "      <td>0.882713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 param_countvectorizer  \\\n",
       "88   CountVectorizer(analyzer='word', binary=False,...   \n",
       "66   CountVectorizer(analyzer='word', binary=False,...   \n",
       "115  CountVectorizer(analyzer='word', binary=False,...   \n",
       "113  CountVectorizer(analyzer='word', binary=False,...   \n",
       "19   CountVectorizer(analyzer='word', binary=False,...   \n",
       "176  CountVectorizer(analyzer='word', binary=False,...   \n",
       "106  CountVectorizer(analyzer='word', binary=False,...   \n",
       "82   CountVectorizer(analyzer='word', binary=False,...   \n",
       "142  CountVectorizer(analyzer='word', binary=False,...   \n",
       "65   CountVectorizer(analyzer='word', binary=False,...   \n",
       "\n",
       "    param_countvectorizer__min_df param_countvectorizer__ngram_range  \\\n",
       "88                              1                             (1, 1)   \n",
       "66                              1                             (1, 1)   \n",
       "115                             1                             (1, 1)   \n",
       "113                             1                             (1, 1)   \n",
       "19                              1                             (1, 1)   \n",
       "176                             1                             (1, 1)   \n",
       "106                             1                             (1, 1)   \n",
       "82                              1                             (1, 1)   \n",
       "142                             1                             (1, 1)   \n",
       "65                              1                             (1, 1)   \n",
       "\n",
       "    param_multinomialnb__alpha  mean_test_score  mean_train_score  \n",
       "88                    0.814801         0.662417          0.879288  \n",
       "66                    0.895266         0.662371          0.876992  \n",
       "115                   0.893602         0.662371          0.877074  \n",
       "113                   0.955471         0.662164          0.875355  \n",
       "19                    0.947308         0.662128          0.875633  \n",
       "176                   0.712271         0.662099          0.883438  \n",
       "106                   0.890044         0.662024          0.877167  \n",
       "82                     0.86904         0.661703          0.877818  \n",
       "142                   0.842561         0.661677          0.878586  \n",
       "65                    0.728939         0.661635          0.882713  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a parameter grid\n",
    "param_distributions = {\n",
    "    'countvectorizer' : [CountVectorizer(), TfidfVectorizer(max_df = 0.8)],\n",
    "    'countvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "    'countvectorizer__min_df' : [1, 2, 3],\n",
    "    'multinomialnb__alpha' : uniform(loc = 0.7, scale = 0.3)\n",
    "}\n",
    "\n",
    "# create a RandomizedSearchCV object\n",
    "nb_random_search = RandomizedSearchCV(\n",
    "    estimator = nb_pipe,\n",
    "    param_distributions = param_distributions,\n",
    "    n_iter = 200,\n",
    "    scoring = 'f1',\n",
    "    n_jobs = -1,\n",
    "    refit = True,\n",
    "    verbose = 1,\n",
    "    random_state = 164,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object\n",
    "nb_random_search.fit(X_train, y_train)\n",
    "\n",
    "# print grid search results\n",
    "cols = ['param_countvectorizer', \n",
    "        'param_countvectorizer__min_df', \n",
    "        'param_countvectorizer__ngram_range', \n",
    "        'param_multinomialnb__alpha', \n",
    "        'mean_test_score', \n",
    "        'mean_train_score']\n",
    "\n",
    "pd.options.display.max_colwidth = 50\n",
    "\n",
    "nb_random_search_results = pd.DataFrame(nb_random_search.cv_results_).sort_values(by = 'mean_test_score', \n",
    "                                                                                  ascending = False)\n",
    "nb_random_search_results[cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set with the best model from the randomized search\n",
    "pred = nb_random_search.predict(X_test)\n",
    "\n",
    "# submit prediction\n",
    "sample_submission.target = pred\n",
    "sample_submission.to_csv('naive_bayes_tuned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Word2Vec & Logistic Regression\n",
    "## Load and Prepare Data\n",
    "Here, I do not remove them because after removing them model performance gets worse. This behavior is unintuitive, especially since there are tweets with the same text but different labels.\n",
    "\n",
    "After that, I create a word embedding for each document using Word2Vec. Word2Vec creates a dense representation for each word, such that words appearing in similar contexts have similar vectors. To get an embedding for the entire tweet, the mean of all vectors for the words in the tweet are taken. The assumption now is that similar tweets have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7613, 5)\n",
      "Test shape: (3263, 4)\n",
      "Sample submission shape: (3263, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n",
    "\n",
    "# print shape of datasets\n",
    "print('Train shape: {}'.format(train.shape))\n",
    "print('Test shape: {}'.format(test.shape))\n",
    "print('Sample submission shape: {}'.format(sample_submission.shape))\n",
    "\n",
    "# inspect train set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set: (7613, 300)\n",
      "Shape of test set: (3263, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_lg model\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "# create train set by getting the document vector\n",
    "docs_train = [nlp(doc).vector for doc in train.text]\n",
    "X_train = np.vstack(docs_train)\n",
    "print('Shape of train set: {}'.format(X_train.shape))\n",
    "\n",
    "# create test set likewise\n",
    "docs_test = [nlp(doc).vector for doc in test.text]\n",
    "X_test = np.vstack(docs_test)\n",
    "print('Shape of test set: {}'.format(X_test.shape))\n",
    "\n",
    "# create target\n",
    "y_train = train.target.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Create Machine Learning Pipeline\n",
    "In this case, it doesn't add much value to use a pipeline since the only step in the pipeline is an estimator (here a logistic regression). However, since it's useful for pipelines with data preprocessing steps that are learned on data, such standard scaling, I even do it when it's not required.\n",
    "\n",
    "However, one advantage even when just using an estimator is that I can treat the estimator like a hyperparameter in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.729\n"
     ]
    }
   ],
   "source": [
    "# create machine learning pipeline\n",
    "word2vec_pipe = Pipeline([('estimator', LogisticRegression())])\n",
    "\n",
    "# cross validate\n",
    "print('F1 score: {:.3f}'.format(np.mean(cross_val_score(word2vec_pipe, X_train, y_train, scoring = 'f1'))))\n",
    "\n",
    "# fit pipeline\n",
    "word2vec_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "pred = word2vec_pipe.predict(X_test)\n",
    "\n",
    "# submit prediction\n",
    "sample_submission.target = pred\n",
    "sample_submission.to_csv('word2vec_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "After creating the baseline, I now want to test if a more complex model works better than the logistic regression. I chose a kernel SVM in this case, as SVM models are one of the classical machine learning models commonly used for text classification.\n",
    "\n",
    "I tune the regularization parameter C for both the logistic regression and SVM and the gamma parameter for the SVM. The hyperparameters influence the model complexity, with more complex models having a higher chance of overfitting. In case of the SVM, a more complex model can even find decision boundaries which are considered non-linear in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 22 candidates, totalling 110 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.5min\n",
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed: 13.0min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_estimator</th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>param_estimator__gamma</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SVC(C=10.0, break_ties=False, cache_size=200, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0131354</td>\n",
       "      <td>0.734215</td>\n",
       "      <td>0.793362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.731396</td>\n",
       "      <td>0.759984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.728846</td>\n",
       "      <td>0.787516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SVC(C=10.0, break_ties=False, cache_size=200, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0131354</td>\n",
       "      <td>0.724366</td>\n",
       "      <td>0.753945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.719089</td>\n",
       "      <td>0.793811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SVC(C=10.0, break_ties=False, cache_size=200, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00131354</td>\n",
       "      <td>0.717796</td>\n",
       "      <td>0.747382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.716332</td>\n",
       "      <td>0.794272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.714957</td>\n",
       "      <td>0.793758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.705592</td>\n",
       "      <td>0.713926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SVC(C=10.0, break_ties=False, cache_size=200, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000131354</td>\n",
       "      <td>0.670485</td>\n",
       "      <td>0.680679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      param_estimator param_estimator__C  \\\n",
       "21  SVC(C=10.0, break_ties=False, cache_size=200, ...                 10   \n",
       "2   LogisticRegression(C=1.0, class_weight=None, d...                0.1   \n",
       "3   LogisticRegression(C=1.0, class_weight=None, d...                  1   \n",
       "16  SVC(C=10.0, break_ties=False, cache_size=200, ...                  1   \n",
       "4   LogisticRegression(C=1.0, class_weight=None, d...                 10   \n",
       "20  SVC(C=10.0, break_ties=False, cache_size=200, ...                 10   \n",
       "5   LogisticRegression(C=1.0, class_weight=None, d...                100   \n",
       "6   LogisticRegression(C=1.0, class_weight=None, d...               1000   \n",
       "1   LogisticRegression(C=1.0, class_weight=None, d...               0.01   \n",
       "19  SVC(C=10.0, break_ties=False, cache_size=200, ...                 10   \n",
       "\n",
       "   param_estimator__gamma  mean_test_score  mean_train_score  \n",
       "21              0.0131354         0.734215          0.793362  \n",
       "2                     NaN         0.731396          0.759984  \n",
       "3                     NaN         0.728846          0.787516  \n",
       "16              0.0131354         0.724366          0.753945  \n",
       "4                     NaN         0.719089          0.793811  \n",
       "20             0.00131354         0.717796          0.747382  \n",
       "5                     NaN         0.716332          0.794272  \n",
       "6                     NaN         0.714957          0.793758  \n",
       "1                     NaN         0.705592          0.713926  \n",
       "19            0.000131354         0.670485          0.680679  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a parameter grid\n",
    "param_grid = [{'estimator' : [LogisticRegression()], \n",
    "               'estimator__C' : np.logspace(-3, 3, 7)},\n",
    "              {'estimator' : [SVC()], \n",
    "               'estimator__C' : np.logspace(-1, 1, 3), \n",
    "               'estimator__gamma' : np.logspace(-2, 2, 5) / X_train.shape[0]}]\n",
    "\n",
    "# create a RandomizedSearchCV object\n",
    "word2vec_grid_search = GridSearchCV(\n",
    "    estimator = word2vec_pipe,\n",
    "    param_grid = param_grid,\n",
    "    scoring = 'f1',\n",
    "    n_jobs = -1,\n",
    "    refit = True,\n",
    "    verbose = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "# fit RandomizedSearchCV object\n",
    "word2vec_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print grid search results\n",
    "cols = ['param_estimator',\n",
    "        'param_estimator__C',\n",
    "        'param_estimator__gamma',\n",
    "        'mean_test_score',\n",
    "        'mean_train_score']\n",
    "\n",
    "pd.options.display.max_colwidth = 50\n",
    "\n",
    "word2vec_grid_search_results = pd.DataFrame(word2vec_grid_search.cv_results_).sort_values(by = 'mean_test_score', \n",
    "                                                                                          ascending = False)\n",
    "word2vec_grid_search_results[cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set with the best model from the randomized search\n",
    "pred = word2vec_grid_search.predict(X_test)\n",
    "\n",
    "# submit prediction\n",
    "sample_submission.target = pred\n",
    "sample_submission.to_csv('word2vec_tuned.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
